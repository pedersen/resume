#ifndef PULSEPOINT_JOB_ALL
#define PULSEPOINT_JOB_ALL 1
#include <functions.gpp>
#define PP_JOB_HEADER ## Pulsepoint - Data Engineer (2015-2017), Director of Infrastructure for Data (2017-current)\
\
New York City, NY (Telecommute) - 2015-Current
#define PP_JOB_SUMMARY Pulsepoint is an internet healthcare marketing company with a focus on activating health care providers. Pulsepoint was acquired by WebMD in June 2021.\
\
My role evolved over time from dealing with individual data jobs to overseeing the entire ETL pipeline to leading the entire department.

#define PP_JOB_BULLET_01 * Architected data streaming that manages 40T of data/day.
#define PP_JOB_BULLET_02 * Established new data centers in Europe and in Virginia.
#define PP_JOB_BULLET_03 * Performance tuned [Kafka][KAFKA].
#define PP_JOB_BULLET_04 * Upgraded [Kafka][KAFKA] with zero downtime for users of [Kafka][KAFKA].
#define PP_JOB_BULLET_05 * Enabled integration with [Active Directory][ACTIVEDIRECTORY] for [Hadoop][HADOOP] systems.
#define PP_JOB_BULLET_06 * Deployed and configured [Alluxio][ALLUXIO] for caching and data orchestration.
#define PP_JOB_BULLET_07 * Troubleshooting of issues with [Hadoop][HADOOP], [Kafka][KAFKA], [SQL Server][MSSQL], and [Kubernetes][KUBERNETES].
#define PP_JOB_BULLET_08 * Developed new ETL jobs to aggregate data from Pulsepoint's RTB exchange.
#define PP_JOB_BULLET_09 * Ingested third party data to make it available internally.
#define PP_JOB_BULLET_10 * Production maintenance of data pipelines, including after hours support.
#define PP_JOB_BULLET_11 * Built tool to graphically show the flow of data through the system.
#define PP_JOB_BULLET_12 * Tested new tools for suitability, including [MariaDB][MARIADB], [Clickhouse][CLICKHOUSE], and [Kudu][KUDU].
#define PP_JOB_BULLET_13 * Installed and configured multiple [Hadoop][HADOOP] clusters.
#define PP_JOB_BULLET_14 * Migrated data center, moving processing of data flows to new data center.
#define PP_JOB_BULLET_15 * Split data management team into data platform and data product development.
#define PP_JOB_BULLET_16 * Insituted and formalized processes and procedures for the team.
#define PP_JOB_BULLET_17 * Planned capacity to ensure we could handle incoming data throughout the year.
#define PP_JOB_BULLET_18 * Replaced [Vertica][VERTICA] with [Trino][TRINO].
#define PP_JOB_BULLET_19 * Reported on system wide data latency using [ElasticSearch][ELASTIC], [Kibana][KIBANA], and [Grafana][GRAFANA].
#define PP_JOB_BULLET_20 * Conducted interviews for my team and for teams that work closely with my team.
#define PP_JOB_BULLET_21 * Automated distribution of incident reports to all affected parties.
#define PP_JOB_BULLET_22 * Changed hardware profiles for [Hadoop][HADOOP] to remove storage and compute colocation.
#define PP_JOB_BULLET_23 * Guided the team through splitting our ETL monolithic repository.
#define PP_JOB_BULLET_24 * Organized the migration of the ETL pipeline from Python 2 to Python 3.
#define PP_JOB_BULLET_25 * Acted as scrum master for the team.
#define PP_JOB_BULLET_26 * Onboarded new team members, helping them to fully integrate into the team.
#define PP_JOB_BULLET_27 * Held weekly 1 on 1 meetings with team members.
#define PP_JOB_BULLET_28 * Participated in on-call rotation.
#define PP_JOB_BULLET_29 * Optimized [Hadoop][HADOOP] jobs.
#define PP_JOB_BULLET_30 * Maintained [Vertica][VERTICA] cluster, including troubleshooting.
#define PP_JOB_BULLET_31 * Transitioned ETL pipeline from crontabs to [Mesos][MESOS] and then into [Kubernetes][KUBERNETES].
#define PP_JOB_BULLET_32 * Switched build server from [TeamCity][TEAMCITY] to [Jenkins][JENKINS], recreating all build jobs.
#define PP_JOB_BULLET_33 * Implemented data duplication between two [Hadoop][HADOOP] clusters.
#define PP_JOB_BULLET_34 * Tested [Cassandra][CASSANDRA] as a potential reporting database.
#define PP_JOB_BULLET_35 * Upgraded [Hadoop][HADOOP] clusters with minimal downtime.

#defeval PP_JOB_BULLETS_BIG_DATA PP_JOB_BULLET_01\
PP_JOB_BULLET_02\
PP_JOB_BULLET_14\
PP_JOB_BULLET_23\
PP_JOB_BULLET_24\
PP_JOB_BULLET_11\
PP_JOB_BULLET_18\
PP_JOB_BULLET_09\
PP_JOB_BULLET_07\
PP_JOB_BULLET_10\
PP_JOB_BULLET_12\
PP_JOB_BULLET_13\
PP_JOB_BULLET_22\
PP_JOB_BULLET_33\
PP_JOB_BULLET_35

#defeval PP_JOB_BULLETS_DEVOPS PP_JOB_BULLET_01\
PP_JOB_BULLET_02\
PP_JOB_BULLET_14\
PP_JOB_BULLET_04\
PP_JOB_BULLET_06\
PP_JOB_BULLET_03\
PP_JOB_BULLET_05\
PP_JOB_BULLET_18\
PP_JOB_BULLET_11\
PP_JOB_BULLET_31\
PP_JOB_BULLET_07\
PP_JOB_BULLET_10\
PP_JOB_BULLET_12\
PP_JOB_BULLET_32\
PP_JOB_BULLET_33\
PP_JOB_BULLET_35\
PP_JOB_BULLET_28

#defeval PP_JOB_BULLETS_DEV PP_JOB_BULLET_01\
PP_JOB_BULLET_02\
PP_JOB_BULLET_14\
PP_JOB_BULLET_08\
PP_JOB_BULLET_09\
PP_JOB_BULLET_23\
PP_JOB_BULLET_24\
PP_JOB_BULLET_29\
PP_JOB_BULLET_33\
PP_JOB_BULLET_28

#defeval PP_JOB_BULLETS_MGR PP_JOB_BULLET_01\
PP_JOB_BULLET_02\
PP_JOB_BULLET_14\
PP_JOB_BULLET_15\
PP_JOB_BULLET_23\
PP_JOB_BULLET_24\
PP_JOB_BULLET_16\
PP_JOB_BULLET_17\
PP_JOB_BULLET_18\
PP_JOB_BULLET_25\
PP_JOB_BULLET_19\
PP_JOB_BULLET_20\
PP_JOB_BULLET_21\
PP_JOB_BULLET_22\
PP_JOB_BULLET_26\
PP_JOB_BULLET_27\
PP_JOB_BULLET_28

#defeval PP_JOB_BULLETS_ALL PP_JOB_BULLET_01\
PP_JOB_BULLET_02\
PP_JOB_BULLET_14\
PP_JOB_BULLET_15\
PP_JOB_BULLET_23\
PP_JOB_BULLET_24\
PP_JOB_BULLET_16\
PP_JOB_BULLET_17\
PP_JOB_BULLET_18\
PP_JOB_BULLET_25\
PP_JOB_BULLET_19\
PP_JOB_BULLET_20\
PP_JOB_BULLET_21\
PP_JOB_BULLET_22\
PP_JOB_BULLET_26\
PP_JOB_BULLET_27\
PP_JOB_BULLET_28\
PP_JOB_BULLET_04\
PP_JOB_BULLET_06\
PP_JOB_BULLET_03\
PP_JOB_BULLET_05\
PP_JOB_BULLET_11\
PP_JOB_BULLET_31\
PP_JOB_BULLET_07\
PP_JOB_BULLET_10\
PP_JOB_BULLET_12\
PP_JOB_BULLET_32\
PP_JOB_BULLET_33\
PP_JOB_BULLET_35\
PP_JOB_BULLET_09\
PP_JOB_BULLET_13\
PP_JOB_BULLET_08\
PP_JOB_BULLET_29\
PP_JOB_BULLET_30\
PP_JOB_BULLET_34

#define PP_JOB_PRJ_NAME_DF     Dataflow Explorer
#define PP_JOB_PRJ_PERIOD_DF   2015
#define PP_JOB_PRJ_COMPANY_DF  Pulsepoint
#define PP_JOB_PRJ_TOOLS_DF    [Python][PYTHON], [Graphviz Dot][GRAPHVIZ], [Luigi][LUIGI]
#define PP_JOB_PRJ_PLATFORM_DF [Mesos][MESOS], [CentOS][CENTOS], [NGINX][NGINX]
#define PP_JOB_PRJ_SUMMARY_DF  At Pulsepoint, we have a large number of data aggregation jobs that are coordinated with each other via Spotify's [Luigi][LUIGI] tool. [Luigi][LUIGI] has the user create a [Python][PYTHON] codebase that resolves which order to do jobs similar to how [GNU Make][MAKE] actually works. A negative side effect of this is difficulty for humans to understand the order of jobs that will be run when the number gets to any significant size.\
\
The Dataflow Explorer would walk the [Python][PYTHON] code that represented all of the jobs, and extract the attributes that would allow construction of a dependency tree. It would then pass that tree to the [Graphviz DOT][GRAPHVIZ] tool, which would run dot to produce an SVG file showing the graph of all the jobs. Finally, it would publish that output onto [Mesos][MESOS] using [NGINX][NGINX], allowing people to browse, zoom, and search the resulting graph.
#define PP_JOB_PRJ_BULLETS_DF  * Wrote code to walk a [Python][PYTHON] code base and extract specific attributes\
* Produced syntactically valid [Dot][GRAPHVIZ] files.\
* Automatically published updated versions of the graph for myself and others to use.

#defeval PP_JOB_PRJ_DATAFLOW_EXPLORER JOB_PROJECT(PP_JOB_PRJ_NAME_DF,PP_JOB_PRJ_PERIOD_DF,PP_JOB_PRJ_COMPANY_DF,PP_JOB_PRJ_TOOLS_DF,PP_JOB_PRJ_PLATFORM_DF,PP_JOB_PRJ_SUMMARY_DF,PP_JOB_PRJ_BULLETS_DF)


#define PP_JOB_PRJ_NAME_NEW_DC Migrate To New Data Center
#define PP_JOB_PRJ_PERIOD_NEW_DC 2022-2023
#define PP_JOB_PRJ_COMPANY_NEW_DC Pulsepoint
#define PP_JOB_PRJ_TOOLS_NEW_DC [Alluxio][ALLUXIO], [Hadoop][HADOOP], [Kafka][KAFKA], [Python][PYTHON]
#define PP_JOB_PRJ_PLATFORM_NEW_DC [CentOS][CENTOS], [Kubernetes][KUBERNETES]
#define PP_JOB_PRJ_SUMMARY_NEW_DC Pulsepoint is in the process of migrating between data centers. A significant portion of the existing hardware has gone past its end of life, so we chose to build a new data center, with new hardware. At the same time, we used the latest versions of all relevant software that we could ([Hadoop][HADOOP], [Kubernetes][KUBERNETES], etc).\
\
This provided us with an opportunity to fix some design flaws in the original big data clusters, and we used this chance to make things better for us overall.\
\
The work remaining at this point comes down to verifying that the new versions of the ETL jobs function as expected, producing valid output. The process is expected to complete in 2025.
#define PP_JOB_PRJ_BULLETS_NEW_DC * Created new clusters, with new versions of relevant software, in the new data center.\
* Updated ETL jobs as needed so that they would run exclusively in the new data center.\
* Configured those ETL jobs to output copies of their data to the original data center.\
* Removed those ETL jobs from the original data center, configuring the original to use the output from the new data center.

#defeval PP_JOB_PRJ_NEW_DC JOB_PROJECT(PP_JOB_PRJ_NAME_NEW_DC,PP_JOB_PRJ_PERIOD_NEW_DC,PP_JOB_PRJ_COMPANY_NEW_DC,PP_JOB_PRJ_TOOLS_NEW_DC,PP_JOB_PRJ_PLATFORM_NEW_DC,PP_JOB_PRJ_SUMMARY_NEW_DC,PP_JOB_PRJ_BULLETS_NEW_DC)


#define PP_JOB_PRJ_NAME_PY3 Migrate From Python 2 to Python 3
#define PP_JOB_PRJ_PERIOD_PY3 2022-2023
#define PP_JOB_PRJ_COMPANY_PY3 Pulsepoint
#define PP_JOB_PRJ_TOOLS_PY3 [Python][PYTHON]
#define PP_JOB_PRJ_PLATFORM_PY3 [CentOS][CENTOS], [Kubernetes][KUBERNETES]
#define PP_JOB_PRJ_SUMMARY_PY3 Pulsepoint built the entire ETL pipeline using [Python][PYTHON] 2. On January 1, 2020, Python 2 reached its end of life. In order for the ETL pipeline to continue to grow, we needed to migrate to Python 3.\
\
The path we chose was to extract the code that was common to the pipeline, and turn that code into a library. We then began the normal route of making backwards incompatible changes. Because of the scope of this work (nearly 200K lines in Python files), and the work being done during a data center migration, the project is still ongoing. However, over 50K lines have been successfully completed so far.
#define PP_JOB_PRJ_BULLETS_PY3 * Established a library cutoff version, after which the library would no longer support Python 2.\
* Began regular release cycles for the library\
* Ensured that developers outside of the library maintenance team could use the library to easily migrate ETL jobs.

#defeval PP_JOB_PRJ_PY3 JOB_PROJECT(PP_JOB_PRJ_NAME_PY3,PP_JOB_PRJ_PERIOD_PY3,PP_JOB_PRJ_COMPANY_PY3,PP_JOB_PRJ_TOOLS_PY3,PP_JOB_PRJ_PLATFORM_PY3,PP_JOB_PRJ_SUMMARY_PY3,PP_JOB_PRJ_BULLETS_PY3)
#endif
