#ifndef PULSEPOINT_JOB_ALL
#define PULSEPOINT_JOB_ALL 1
#include <functions.gpp>
#define PP_JOB_HEADER ## Pulsepoint - Data Engineer (2015-2018), Director of Infrastructure for Data (2018-current)\
\
New York City, NY (Telecommute) - 2015-Current
#define PP_JOB_SUMMARY Pulsepoint is an internet healthcare marketing company with a focus on activating health care providers. Pulsepoint was acquired by WebMD in June 2021.\
\
My role evolved over time from dealing with individual data jobs to overseeing the entire ETL pipeline to leading the entire department.

#define PP_JOB_BULLET_01 * Architected data streaming that manages 40T of data/day.
#define PP_JOB_BULLET_02 * Established new data centers in Europe and in Virginia.
#define PP_JOB_BULLET_03 * Performance tuned [Kafka][KAFKA].
#define PP_JOB_BULLET_04 * Upgraded [Kafka][KAFKA] with zero downtime for users of [Kafka][KAFKA].
#define PP_JOB_BULLET_05 * Enabled integration with [Active Directory][ACTIVEDIRECTORY] for [Hadoop][HADOOP] systems.
#define PP_JOB_BULLET_06 * Deployed and configured [Alluxio][ALLUXIO] for caching and data orchestration.
#define PP_JOB_BULLET_07 * Troubleshooting of issues with [Hadoop][HADOOP], [Kafka][KAFKA], [SQL Server][MSSQL], and [Kubernetes][KUBERNETES].
#define PP_JOB_BULLET_08 * Developed new ETL jobs to aggregate data from Pulsepoint's RTB exchange.
#define PP_JOB_BULLET_09 * Ingested third party data to make it available internally.
#define PP_JOB_BULLET_10 * Production maintenance of data pipelines, including after hours support.
#define PP_JOB_BULLET_11 * Built tool to graphically show the flow of data through the system.
#define PP_JOB_BULLET_12 * Tested new tools for suitability, including [MariaDB][MARIADB], [Clickhouse][CLICKHOUSE], and [Kudu][KUDU].
#define PP_JOB_BULLET_13 * Installed and configured multiple [Hadoop][HADOOP] clusters.
#define PP_JOB_BULLET_14 * Migrated data center, moving processing of data flows to new data center.
#define PP_JOB_BULLET_15 * Split data management team into data platform and data product development.
#define PP_JOB_BULLET_16 * Insituted and formalized processes and procedures for the team.
#define PP_JOB_BULLET_17 * Planned capacity to ensure we could handle incoming data throughout the year.
#define PP_JOB_BULLET_18 * Replaced [Vertica][VERTICA] with [Trino][TRINO].
#define PP_JOB_BULLET_19 * Reported on system wide data latency using [ElasticSearch][ELASTIC], [Kibana][KIBANA], and [Grafana][GRAFANA].
#define PP_JOB_BULLET_20 * Conducted interviews for my team and for teams that work closely with my team.
#define PP_JOB_BULLET_21 * Automated distribution of incident reports to all affected parties.
#define PP_JOB_BULLET_22 * Changed hardware profiles for [Hadoop][HADOOP] to remove storage and compute colocation.
#define PP_JOB_BULLET_23 * Guided the team through splitting our ETL monolithic repository.
#define PP_JOB_BULLET_24 * Organized the migration of the ETL pipeline from Python 2 to Python 3.
#define PP_JOB_BULLET_25 * Acted as scrum master for the team.
#define PP_JOB_BULLET_26 * Onboarded new team members, helping them to fully integrate into the team.
#define PP_JOB_BULLET_27 * Held weekly 1 on 1 meetings with team members.
#define PP_JOB_BULLET_28 * Participated in on-call rotation.
#define PP_JOB_BULLET_29 * Optimized [Hadoop][HADOOP] jobs.
#define PP_JOB_BULLET_30 * Maintained [Vertica][VERTICA] cluster, including troubleshooting.
#define PP_JOB_BULLET_31 * Transitioned ETL pipeline from crontabs to [Mesos][MESOS] and then into [Kubernetes][KUBERNETES].
#define PP_JOB_BULLET_32 * Switched build server from [TeamCity][TEAMCITY] to [Jenkins][JENKINS], recreating all build jobs.
#define PP_JOB_BULLET_33 * Implemented data duplication between two [Hadoop][HADOOP] clusters.
#define PP_JOB_BULLET_34 * Tested [Cassandra][CASSANDRA] as a potential reporting database.
#define PP_JOB_BULLET_35 * Upgraded [Hadoop][HADOOP] clusters with minimal downtime.
#define PP_JOB_BULLET_36 * Passed annual HIPAA training for data protection.
#define PP_JOB_BULLET_37 * Developed new stories (including estimates) for our Jira board.
#define PP_JOB_BULLET_38 * Prioritized tickets for our Jira board.

#defeval PP_JOB_BULLETS_BIG_DATA PP_JOB_BULLET_01\
PP_JOB_BULLET_02\
PP_JOB_BULLET_14\
PP_JOB_BULLET_23\
PP_JOB_BULLET_24\
PP_JOB_BULLET_11\
PP_JOB_BULLET_18\
PP_JOB_BULLET_09\
PP_JOB_BULLET_07\
PP_JOB_BULLET_10\
PP_JOB_BULLET_12\
PP_JOB_BULLET_13\
PP_JOB_BULLET_22\
PP_JOB_BULLET_33\
PP_JOB_BULLET_35\
PP_JOB_BULLET_36\
PP_JOB_BULLET_34\
PP_JOB_BULLET_31\
PP_JOB_BULLET_19

#defeval PP_JOB_BULLETS_DEVOPS PP_JOB_BULLET_01\
PP_JOB_BULLET_02\
PP_JOB_BULLET_14\
PP_JOB_BULLET_04\
PP_JOB_BULLET_06\
PP_JOB_BULLET_03\
PP_JOB_BULLET_05\
PP_JOB_BULLET_18\
PP_JOB_BULLET_11\
PP_JOB_BULLET_31\
PP_JOB_BULLET_07\
PP_JOB_BULLET_10\
PP_JOB_BULLET_12\
PP_JOB_BULLET_32\
PP_JOB_BULLET_33\
PP_JOB_BULLET_35\
PP_JOB_BULLET_28

#defeval PP_JOB_BULLETS_DEV PP_JOB_BULLET_01\
PP_JOB_BULLET_02\
PP_JOB_BULLET_14\
PP_JOB_BULLET_08\
PP_JOB_BULLET_09\
PP_JOB_BULLET_23\
PP_JOB_BULLET_24\
PP_JOB_BULLET_29\
PP_JOB_BULLET_33\
PP_JOB_BULLET_28

#defeval PP_JOB_BULLETS_MGR PP_JOB_BULLET_01\
PP_JOB_BULLET_02\
PP_JOB_BULLET_14\
PP_JOB_BULLET_15\
PP_JOB_BULLET_23\
PP_JOB_BULLET_24\
PP_JOB_BULLET_16\
PP_JOB_BULLET_17\
PP_JOB_BULLET_18\
PP_JOB_BULLET_25\
PP_JOB_BULLET_19\
PP_JOB_BULLET_20\
PP_JOB_BULLET_21\
PP_JOB_BULLET_22\
PP_JOB_BULLET_26\
PP_JOB_BULLET_27\
PP_JOB_BULLET_28\
PP_JOB_BULLET_37\
PP_JOB_BULLET_38\
PP_JOB_BULLET_36

#defeval PP_JOB_BULLETS_ALL PP_JOB_BULLET_01\
PP_JOB_BULLET_02\
PP_JOB_BULLET_14\
PP_JOB_BULLET_15\
PP_JOB_BULLET_23\
PP_JOB_BULLET_24\
PP_JOB_BULLET_16\
PP_JOB_BULLET_17\
PP_JOB_BULLET_18\
PP_JOB_BULLET_25\
PP_JOB_BULLET_19\
PP_JOB_BULLET_20\
PP_JOB_BULLET_21\
PP_JOB_BULLET_22\
PP_JOB_BULLET_26\
PP_JOB_BULLET_27\
PP_JOB_BULLET_28\
PP_JOB_BULLET_37\
PP_JOB_BULLET_38\
PP_JOB_BULLET_36\
PP_JOB_BULLET_04\
PP_JOB_BULLET_06\
PP_JOB_BULLET_03\
PP_JOB_BULLET_05\
PP_JOB_BULLET_11\
PP_JOB_BULLET_31\
PP_JOB_BULLET_07\
PP_JOB_BULLET_10\
PP_JOB_BULLET_12\
PP_JOB_BULLET_32\
PP_JOB_BULLET_33\
PP_JOB_BULLET_35\
PP_JOB_BULLET_09\
PP_JOB_BULLET_13\
PP_JOB_BULLET_08\
PP_JOB_BULLET_29\
PP_JOB_BULLET_30\
PP_JOB_BULLET_34

#define PP_JOB_PRJ_NAME_DF     Dataflow Explorer
#define PP_JOB_PRJ_PERIOD_DF   2015
#define PP_JOB_PRJ_COMPANY_DF  Pulsepoint
#define PP_JOB_PRJ_TOOLS_DF    [Python][PYTHON], [Graphviz Dot][GRAPHVIZ], [Luigi][LUIGI]
#define PP_JOB_PRJ_PLATFORM_DF [Mesos][MESOS], [CentOS][CENTOS], [NGINX][NGINX]
#define PP_JOB_PRJ_SUMMARY_DF  At Pulsepoint, we have a large number of data aggregation jobs that are coordinated with each other via Spotify's [Luigi][LUIGI] tool. [Luigi][LUIGI] has the user create a [Python][PYTHON] codebase that resolves which order to do jobs similar to how [GNU Make][MAKE] actually works. A negative side effect of this is difficulty for humans to understand the order of jobs that will be run when the number gets to any significant size.\
\
The Dataflow Explorer would walk the [Python][PYTHON] code that represented all of the jobs, and extract the attributes that would allow construction of a dependency tree. It would then pass that tree to the [Graphviz DOT][GRAPHVIZ] tool, which would run dot to produce an SVG file showing the graph of all the jobs. Finally, it would publish that output onto [Mesos][MESOS] using [NGINX][NGINX], allowing people to browse, zoom, and search the resulting graph.
#define PP_JOB_PRJ_BULLETS_DF  * Wrote code to walk a [Python][PYTHON] code base and extract specific attributes\
* Produced syntactically valid [Dot][GRAPHVIZ] files.\
* Automatically published updated versions of the graph for myself and others to use.

#defeval PP_JOB_PRJ_DATAFLOW_EXPLORER JOB_PROJECT(PP_JOB_PRJ_NAME_DF,PP_JOB_PRJ_PERIOD_DF,PP_JOB_PRJ_COMPANY_DF,PP_JOB_PRJ_TOOLS_DF,PP_JOB_PRJ_PLATFORM_DF,PP_JOB_PRJ_SUMMARY_DF,PP_JOB_PRJ_BULLETS_DF)


#define PP_JOB_PRJ_NAME_NEW_DC Migrate To New Data Center
#define PP_JOB_PRJ_PERIOD_NEW_DC 2022-2023
#define PP_JOB_PRJ_COMPANY_NEW_DC Pulsepoint
#define PP_JOB_PRJ_TOOLS_NEW_DC [Alluxio][ALLUXIO], [Hadoop][HADOOP], [Kafka][KAFKA], [Python][PYTHON]
#define PP_JOB_PRJ_PLATFORM_NEW_DC [CentOS][CENTOS], [Kubernetes][KUBERNETES]
#define PP_JOB_PRJ_SUMMARY_NEW_DC Pulsepoint is in the process of migrating between data centers. A significant portion of the existing hardware has gone past its end of life, so we chose to build a new data center, with new hardware. At the same time, we used the latest versions of all relevant software that we could ([Hadoop][HADOOP], [Kubernetes][KUBERNETES], etc).\
\
This provided us with an opportunity to fix some design flaws in the original big data clusters, and we used this chance to make things better for us overall.\
\
The work remaining at this point comes down to verifying that the new versions of the ETL jobs function as expected, producing valid output. The process is expected to complete in 2025.
#define PP_JOB_PRJ_BULLETS_NEW_DC * Created new clusters, with new versions of relevant software, in the new data center.\
* Updated ETL jobs as needed so that they would run exclusively in the new data center.\
* Configured those ETL jobs to output copies of their data to the original data center.\
* Removed those ETL jobs from the original data center, configuring the original to use the output from the new data center.

#defeval PP_JOB_PRJ_NEW_DC JOB_PROJECT(PP_JOB_PRJ_NAME_NEW_DC,PP_JOB_PRJ_PERIOD_NEW_DC,PP_JOB_PRJ_COMPANY_NEW_DC,PP_JOB_PRJ_TOOLS_NEW_DC,PP_JOB_PRJ_PLATFORM_NEW_DC,PP_JOB_PRJ_SUMMARY_NEW_DC,PP_JOB_PRJ_BULLETS_NEW_DC)


#define PP_JOB_PRJ_NAME_PY3 Migrate From Python 2 to Python 3
#define PP_JOB_PRJ_PERIOD_PY3 2022-2023
#define PP_JOB_PRJ_COMPANY_PY3 Pulsepoint
#define PP_JOB_PRJ_TOOLS_PY3 [Python][PYTHON]
#define PP_JOB_PRJ_PLATFORM_PY3 [CentOS][CENTOS], [Kubernetes][KUBERNETES]
#define PP_JOB_PRJ_SUMMARY_PY3 Pulsepoint built the entire ETL pipeline using [Python][PYTHON] 2. On January 1, 2020, Python 2 reached its end of life. In order for the ETL pipeline to continue to grow, we needed to migrate to Python 3.\
\
The path we chose was to extract the code that was common to the pipeline, and turn that code into a library. We then began the normal route of making backwards incompatible changes. Because of the scope of this work (nearly 200K lines in Python files), and the work being done during a data center migration, the project is still ongoing. However, over 50K lines have been successfully completed so far.
#define PP_JOB_PRJ_BULLETS_PY3 * Established a library cutoff version, after which the library would no longer support Python 2.\
* Began regular release cycles for the library\
* Ensured that developers outside of the library maintenance team could use the library to easily migrate ETL jobs.

#defeval PP_JOB_PRJ_PY3 JOB_PROJECT(PP_JOB_PRJ_NAME_PY3,PP_JOB_PRJ_PERIOD_PY3,PP_JOB_PRJ_COMPANY_PY3,PP_JOB_PRJ_TOOLS_PY3,PP_JOB_PRJ_PLATFORM_PY3,PP_JOB_PRJ_SUMMARY_PY3,PP_JOB_PRJ_BULLETS_PY3)

#define PP_JOB_PRJ_NAME_CASSANDRA Cassandra for User Reporting
#define PP_JOB_PRJ_PERIOD_CASSANDRA 2015
#define PP_JOB_PRJ_COMPANY_CASSANDRA Pulsepoint
#define PP_JOB_PRJ_TOOLS_CASSANDRA [Cassandra][CASSANDRA]
#define PP_JOB_PRJ_PLATFORM_CASSANDRA [CentOS Linux][CENTOS]
#define PP_JOB_PRJ_SUMMARY_CASSANDRA Pulsepoint has a fairly significant [Microsoft SQL Server][MSSQL] installation, and we were asked if we could use [Cassandra][CASSANDRA] as a replacement for it. We set up a small cluster, and began trying to run various reports against it.\
\
The actual performance was impressive, but we ran into a significant roadblock: Cassandra is, in significant ways, a disk based key/value store. In order to use this as a reporting database, and avoid triggering table scans for the user reporting, we would have had to load up many copies of the same data into different tables with different primary keys.\
\
In the end, this was deemed non-feasible for the number of combinations we would have had to provide, along with the amount of maintenance as new reports could be brought online.

#define PP_JOB_PRJ_BULLETS_CASSANDRA * Deployed a [Cassandra][CASSANDRA] cluster.\
* Produced data sets into that cluster.\
* Confirmed queries ran, and ran well.\
* Ultimately recommended against because of the issues with table scans and primary keys.

#defeval PP_JOB_PRJ_CASSANDRA JOB_PROJECT(PP_JOB_PRJ_NAME_CASSANDRA, PP_JOB_PRJ_PERIOD_CASSANDRA, PP_JOB_PRJ_COMPANY_CASSANDRA, PP_JOB_PRJ_TOOLS_CASSANDRA, PP_JOB_PRJ_PLATFORM_CASSANDRA, PP_JOB_PRJ_SUMMARY_CASSANDRA, PP_JOB_PRJ_BULLETS_CASSANDRA)

#define PP_JOB_PRJ_NAME_DATA_HUB California Hadoop Cluster
#define PP_JOB_PRJ_PERIOD_DATA_HUB 2015
#define PP_JOB_PRJ_COMPANY_DATA_HUB Pulsepoint
#define PP_JOB_PRJ_TOOLS_DATA_HUB [Hadoop][HADOOP]
#define PP_JOB_PRJ_PLATFORM_DATA_HUB [CentOS Linux][CENTOS]
#define PP_JOB_PRJ_SUMMARY_DATA_HUB Pulsepoint needed to establish a disaster recovery site, and had chosen an existing data center to do so. In the process, establishing a [Hadoop][HADOOP] cluster was required for business continuity. My task was to get everything configured to the point that the same data jobs running in the primary cluster ran in the backup cluster and provided equivalent data, even though everything was running independently.
#define PP_JOB_PRJ_BULLETS_DATA_HUB * Installed [Cloudera Distribution of Hadoop][CDH] across the cluster.\
* Ensured that [HDFS][HADOOP], [Hive][HIVE] and [Impala][IMPALA] were functioning properly.\
* Ensured that the same data jobs running in the primary cluster were running in the secondary cluster.\
* Ensured that equivalent output was happening in both data centers.

#defeval PP_JOB_PRJ_DATA_HUB JOB_PROJECT(PP_JOB_PRJ_NAME_DATA_HUB, PP_JOB_PRJ_PERIOD_DATA_HUB, PP_JOB_PRJ_COMPANY_DATA_HUB, PP_JOB_PRJ_TOOLS_DATA_HUB, PP_JOB_PRJ_PLATFORM_DATA_HUB, PP_JOB_PRJ_SUMMARY_DATA_HUB, PP_JOB_PRJ_BULLETS_DATA_HUB)

#define PP_JOB_PRJ_NAME_BCP [Sqoop][SQOOP] to FreeBCP([FreeTDS][FREETDS]) Conversion
#define PP_JOB_PRJ_PERIOD_BCP 2016
#define PP_JOB_PRJ_COMPANY_BCP Pulsepoint
#define PP_JOB_PRJ_TOOLS_BCP [Sqoop][SQOOP], [FreeTDS][FREETDS]
#define PP_JOB_PRJ_PLATFORM_BCP [Hadoop][HADOOP], [Microsoft SQL Server][MSSQL]
#define PP_JOB_PRJ_SUMMARY_BCP [Apache Sqoop][SQOOP] has long been deprecated, with its eventual complete retirement in June 2021. As part of Pulsepoint's platform, we needed a replacement for [Sqoop][SQOOP] before it was fully retired. We settled on FreeBCP, which is part of the [FreeTDS][FREETDS] project. Using this tool, we were able to migrate our processes for transferring data from [Hadoop][HADOOP] to [MS SQL Server][MSSQL].
#define PP_JOB_PRJ_BULLETS_BCP * Developed migration strategy to transition from [Sqoop][SQOOP] to FreeBCP.\
* Tested FreeBCP as a substitue for [Sqoop][SQOOP].\
* Updated our ETL pipelines to use FreeBCP in place of [Sqoop][SQOOP].

#defeval PP_JOB_PRJ_BCP JOB_PROJECT(PP_JOB_PRJ_NAME_BCP, PP_JOB_PRJ_PERIOD_BCP, PP_JOB_PRJ_COMPANY_BCP, PP_JOB_PRJ_TOOLS_BCP, PP_JOB_PRJ_PLATFORM_BCP, PP_JOB_PRJ_SUMMARY_BCP, PP_JOB_PRJ_BULLETS_BCP)

#define PP_JOB_PRJ_NAME_VERTICA [Vertica][VERTICA] Decommissioning
#define PP_JOB_PRJ_PERIOD_VERTICA 2018
#define PP_JOB_PRJ_COMPANY_VERTICA Pulsepoint
#define PP_JOB_PRJ_TOOLS_VERTICA [Vertica][VERTICA], [Trino][TRINO]
#define PP_JOB_PRJ_PLATFORM_VERTICA [CentOS Linux][CENTOS]
#define PP_JOB_PRJ_SUMMARY_VERTICA Pulsepoint had used [Vertica][VERTICA], but we were outgrowing it in 2017. In 2018, when we came up for the most recent support renewal, we had fully outgrown it and needed to replace it with something else. After trying out several other options (including [Clickhouse][CLICKHOUSE], [Trino][TRINO], [MariaDB][MARIOADB], and others), we settled on Trino as the option that provided us with the best capabilities while being nearest to the performance that [Vertica][VERTICA] provided.
#define PP_JOB_PRJ_BULLETS_VERTICA * Performance tested existing [Vertica][VERTICA] queries.\
* Stood up several competitors and compared their performance using the same queries.\
* Compared maintenance of these environments to Vertica.\
* Finally chose [Trino][TRINO], implemented it, and fully decommissioned [Vertica][VERTICA].

#defeval PP_JOB_PRJ_VERTICA JOB_PROJECT(PP_JOB_PRJ_NAME_VERTICA, PP_JOB_PRJ_PERIOD_VERTICA, PP_JOB_PRJ_COMPANY_VERTICA, PP_JOB_PRJ_TOOLS_VERTICA, PP_JOB_PRJ_PLATFORM_VERTICA, PP_JOB_PRJ_SUMMARY_VERTICA, PP_JOB_PRJ_BULLETS_VERTICA)

#define PP_JOB_PRJ_NAME_DM_SPLIT Data Management Team Split
#define PP_JOB_PRJ_PERIOD_DM_SPLIT 2021
#define PP_JOB_PRJ_COMPANY_DM_SPLIT Pulsepoint
#define PP_JOB_PRJ_TOOLS_DM_SPLIT [Git][GIT]
#define PP_JOB_PRJ_PLATFORM_DM_SPLIT [Jira][JIRA], [GitHub][GITHUB]
#define PP_JOB_PRJ_SUMMARY_DM_SPLIT As part of the growth of Pulsepoint, the Data Management team reached a point wherein the team was no longer able to do everything that was required: New data products were needed, and the data platform itself needed both maintenance and new features as well. I made the decision to split the team in two, creating a Data Platform team and a Data Product Development team. Each team would be focused on exactly one role, instead of trying to split the focus between two distinct functions.
#define PP_JOB_PRJ_BULLETS_DM_SPLIT * Divided the team into two distinct functional teams.\
* Divided the code between the two teams to reflect their individual functions.\
* Divided the [Jira][JIRA] board between the two teams.\
* Established new teams on [GitHub][GITHUB], with each team getting only the portion of the code belonging to them.

#defeval PP_JOB_PRJ_DM_SPLIT JOB_PROJECT(PP_JOB_PRJ_NAME_DM_SPLIT, PP_JOB_PRJ_PERIOD_DM_SPLIT, PP_JOB_PRJ_COMPANY_DM_SPLIT, PP_JOB_PRJ_TOOLS_DM_SPLIT, PP_JOB_PRJ_PLATFORM_DM_SPLIT, PP_JOB_PRJ_SUMMARY_DM_SPLIT, PP_JOB_PRJ_BULLETS_DM_SPLIT)

#define PP_JOB_PRJ_NAME_REPO_SPLIT Data Management Code Split
#define PP_JOB_PRJ_PERIOD_REPO_SPLIT 2021
#define PP_JOB_PRJ_COMPANY_REPO_SPLIT Pulsepoint
#define PP_JOB_PRJ_TOOLS_REPO_SPLIT [Git][GIT]
#define PP_JOB_PRJ_PLATFORM_REPO_SPLIT [GitHub][GITHUB]
#define PP_JOB_PRJ_SUMMARY_REPO_SPLIT Pulsepoint needed to split the Data Management team into a Data Platform team and a Data Product Development team. This also meant splitting the code, since the entirety of the ETL pipeline was in one monolithic repository. The team had to develop a means of crossing repository boundaries to establish the pipeline steps (e.g.: Job A in repository 1 is dependent on Job B in repository 2). We also had to come to agreements on how to determine which team got which pieces of code.
#define PP_JOB_PRJ_BULLETS_REPO_SPLIT * Developed cross-repository dependency system for ETL jobs.\
* Agreed on terms to decide which team got which piece of code from the original repository.\
* Created new repositories to get that code.\
* Created new teams on [GitHub][GITHUB] to assign ownership over the newly divided code.

#defeval PP_JOB_PRJ_REPO_SPLIT JOB_PROJECT(PP_JOB_PRJ_NAME_REPO_SPLIT, PP_JOB_PRJ_PERIOD_REPO_SPLIT, PP_JOB_PRJ_COMPANY_REPO_SPLIT, PP_JOB_PRJ_TOOLS_REPO_SPLIT, PP_JOB_PRJ_PLATFORM_REPO_SPLIT, PP_JOB_PRJ_SUMMARY_REPO_SPLIT, PP_JOB_PRJ_BULLETS_REPO_SPLIT)
#endif
