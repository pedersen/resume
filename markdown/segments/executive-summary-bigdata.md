# Summary

I'm a data engineer with 9 years of experience. I've scaled up a company from receiving 4T of new data/day through to its current 40T/day of new data. We have grown from about 50 servers to 450 servers for our big data architecture, as well as holding over 2P of data we are storing and actively using.

I have worked with streaming challenges, ETL challenges, and data latency challenges as we keep our overall latency to 5 hours or less. We have done this using [Hadoop][HADOOP], [Python][PYTHON], [Kafka][KAFKA], and [Alluxio][Alluxio]. We have worked with different data storage formats, different compute engines for accessing the data, and different ways to coordinate our ETL pipeline to avoid having jobs crash into each other.

Most importantly, all of this has been done while keeping an eye on our budget, making sure to keep our costs down to keep the company profitable right through COVID-19.
